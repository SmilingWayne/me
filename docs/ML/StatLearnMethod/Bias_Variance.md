# 关于 Bias 和 Variance



### 核心答案

*   **Boosting 主要旨在降低偏差 (Bias)。**
*   **Bagging 主要旨在降低方差 (Variance)。**

### 详细解释

要理解这一点，我们首先要回顾一下偏差和方差的含义：

*   **偏差 (Bias)**：描述的是模型的**平均预测值**与**真实值**之间的差距。高偏差意味着模型过于简单，没有学习到数据中的潜在规律，导致**欠拟合 (Underfitting)**。比如，用一条直线去拟合一个二次函数曲线。
*   **方差 (Variance)**：描述的是模型在不同训练集上的预测结果的**波动性或离散程度**。高方差意味着模型过于复杂，对训练数据中的噪声非常敏感，导致**过拟合 (Overfitting)**。比如，用一个高次多项式去完美穿过所有训练数据点。

**机器学习的目标是在偏差和方差之间找到一个平衡点（Bias-Variance Trade-off）。**

#### 为什么 Boosting 降低偏差？

Boosting 的工作方式是**串行**的、**循序渐进**的。它通过构建一系列的**弱学习器 (Weak Learners)**，然后将它们加权组合起来。

1.  **从高偏差开始**：Boosting 通常使用的基学习器是结构非常简单的模型，比如决策树桩（只有一层分裂的决策树）。这些弱学习器本身具有**高偏差和低方差**的特点。它们很简单，所以无法很好地拟合数据（高偏差），但也正因为简单，所以对数据的扰动不敏感（低方差）。

2.  **专注修正错误**：Boosting 的核心思想是，**后续的模型要重点关注和修正之前模型犯下的错误**。无论是 AdaBoost（提高错分样本权重）还是 GBDT（拟合残差），每一轮新的学习器都是在努力**弥补**当前集成模型的**不足之处（即偏差）**。

3.  **组合成强学习器**：通过一轮一轮地迭代，模型不断地在之前的基础上进行精炼，逐步降低整体的偏差。最终，许多高偏差的弱学习器组合成了一个**低偏差**的强学习器，它能够很好地拟合训练数据。

**总结**：Boosting 像一个学生做题，第一遍做完发现错了很多（高偏差），于是针对错题再做一遍，又发现新的错误，再针对新的错题做一遍... 通过不断地查漏补缺，最终把所有知识点都学会了，综合能力（拟合能力）大大提高，偏差也就降低了。

#### 为什么 Bagging 降低方差？

Bagging (Bootstrap Aggregating) 的工作方式是**并行**的。它通过对原始数据集进行有放回的抽样（Bootstrap）来创建多个不同的训练子集，然后在每个子集上独立地训练一个模型。

1.  **从低偏差开始**：Bagging 通常使用的基学习器是**没有经过剪枝的决策树**等复杂模型。这些模型本身具有**低偏差和高方差**的特点。它们足够复杂，可以很好地拟合各自的训练子集（低偏差），但由于对数据很敏感，它们在不同的子集上训练出来的结果差异很大（高方差），即模型非常不稳定。

2.  **投票/平均来降低波动**：Bagging 的核心思想是**“三个臭皮匠，顶个诸葛亮”**。它将所有独立训练出的、高方差的模型的结果进行**投票（分类）或平均（回归）**。这个过程可以有效地**抵消掉**单个模型由于数据扰动而产生的极端预测值。

    这在统计学上是有道理的：假设我们有很多独立的随机变量，它们的方差都是 $\sigma^2$。那么，它们的平均值的方差会降低到 $\sigma^2 / N$，其中 $N$ 是变量的数量。虽然 Bagging 中的模型不完全独立，但自助采样法和随机森林中的特征随机化都在努力降低模型间的相关性，使得这种“平均降方差”的效果得以实现。

**总结**：Bagging 像是一个专家委员会做决策。每个专家（复杂的决策树）都很厉害，但可能在某些方面有自己独特的偏见和过度解读（高方差）。通过让所有专家独立分析后投票表决，那些个人的、极端的、不稳定的看法就被平均掉了，最终得到一个非常稳健、可靠的集体决策（低方差）。

---

### 如何记忆？

你可以通过以下两个角度来帮助记忆：

**1. 从基学习器的特点出发**

*   **Boosting**：使用**弱学习器**（High Bias, Low Variance）。它的目标是把一堆“弱鸡”组合成“超人”，这个提升的过程主要是在弥补“弱”这个属性，也就是**降低 Bias**。
*   **Bagging**：使用**强学习器**（Low Bias, High Variance）。它的目标是把一堆“学神”（但有点神经质，不稳定）的意见综合起来，让最终结果更稳定，也就是**降低 Variance**。

**2. 从算法流程的逻辑出发**

*   **Boosting (串行合作)**：像一个**“精益求精的团队”**。A 做完，B 在 A 的基础上改进，C 再在 B 的基础上改进。整个团队的目标是把最初那个粗糙的方案（High Bias）打磨得越来越完美。
*   **Bagging (并行独立)**：像一个**“民主投票的议会”**。议会里的每个成员（模型）都独立思考，可能会有各种激进的想法（High Variance）。但最终的决策是靠投票，中和了所有极端意见，结果非常稳健。

下面这个表格可以帮你巩固记忆：

| 特性         | Bagging                   | Boosting                  |
| :----------- | :------------------------ | :------------------------ |
| **主要目标** | **降低方差 (Variance)**   | **降低偏差 (Bias)**       |
| **基学习器** | 复杂模型 (低偏差, 高方差) | 简单模型 (高偏差, 低方差) |
| **模型关系** | 并行，相互独立            | 串行，相互依赖            |
| **核心思想** | 投票或平均                | 专注修正错误              |
| **代表算法** | 随机森林 (Random Forest)  | AdaBoost, GBDT, XGBoost   |