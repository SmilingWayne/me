# KNN: K-nearest Neighbours

工作机制：工作机制：给定测试样本，基于某种距离度量找到训练集中和其最靠近的 k 个训练样本，然后基于这 $k$ 个“邻居”的信息进行预测。

> 如果 K=1，也就是按照 1-NN 的思路进行，最近邻分类器的泛化错误不超过贝叶斯最优分类器的错误率的两倍。

!!! note "细节"
    1．$k$近邻法是基本且简单的分类与回归方法。$k$近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的$k$个最近邻训练实例点，然后利用这$k$个训练实例点的类的多数来预测输入实例点的类。

    2．$k$近邻模型对应于基于训练数据集对特征空间的一个划分。$k$近邻法中，**当训练集、距离度量、$k$值、分类决策规则确定后，其结果唯一确定。**

    3．$k$近邻法三要素：**距离度量、$k$值的选择和分类决策规则**。常用的距离度量是欧氏距离及更一般的**pL**距离。$k$值小时，$k$近邻模型更复杂；$k$值大时，$k$近邻模型更简单。$k$值的选择反映了对近似误差与估计误差之间的权衡，通常由**交叉验证**选择最优的$k$。

    常用的分类决策规则是多数表决，对应于经验风险最小化。

    4．$k$近邻法的实现需要**考虑如何快速搜索k个最近邻点**。**kd**树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对$k$维空间的一个划分，其每个结点对应于$k$维空间划分中的一个超矩形区域。利用**kd**树可以省去对大部分数据点的搜索， 从而减少搜索的计算量。


## 距离度量

设特征空间$x$是$n$维实数向量空间 ，$x_{i}, x_{j} \in \mathcal{X}$,$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$,$x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}$
，则：$x_i$,$x_j$的$L_p$距离定义为:


$L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{k=1}^{n}\left|x_{i}^{(k)}-x_{j}^{(k)}\right|^{p}\right)^{\frac{1}{p}}$

- $p= 1$  曼哈顿距离
- $p= 2$  欧氏距离
- $p= \infty$   切比雪夫距离

## 手撕细节

维护一个K-最大的列表，记录最小的K个值和tag，对于其他的点，如果比最大的还小，就用index方法替换掉最小的那个值，始终保持这个长度为K的列表里是最小的前3个值。

> `np.linalg.norm` 方法.

## KD-Tree 数据结构

适用于快速找到最接近的多个邻居。缺点是“有可能会遗漏最近的”。因为会对整个多维空间进行划分。优点是整个树最多只有 $\log_2M$ 层，可以加速搜索。

![](https://cdn.jsdelivr.net/gh/SmilingWayne/picsrepo/202411081042708.png)

