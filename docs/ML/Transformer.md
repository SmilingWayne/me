# Transformer

ML：不要在代码中明确定义如何执行一个任务，而是去构建一个具有可调参数的灵活架构，就像旋钮一样，然后用大量实例告诉它给定一个输入的时候应该输出什么，设法调整各种参数值来模拟这种行为。

最简单的ML的例子就是线性回归，当然你只需要拟合两个参数，斜率和常项，GPT-3就需要**拟合1750亿个参数。**

但是并不只是直接扩大参数量，而是说需要用灵活的方法进行训练：**反向传播（Back- propagation）**

为了这种训练算法的有效运行，模型必须遵循某种特定结构。

## Token

某个单词/单词的一部分，或者符号标点。我们首先有一个很大的词库，比如 50k 个token。我们引入第一个Matrix: Embedding Matrix。每一个词（Token）都对应一列，**这些列决定了第一步中，每个单词对应的向量。我们记为 $W_E$** 。这个矩阵的取值一开始是随机的，但是会基于数据进行学习。词嵌入的过程：用一个方向来编码语义信息等。以GPT-3为例，每个Token 有 12288 个维度，50257个Token。也就是 6.17 亿个参数。


对于Transformer而言，我们不仅能够直接得到Embedding中每个单词的向量，**还需要编码单词的位置信息，更重要的是，这些向量能够结合上下文语境。** 也就是被输入的语句中的其他Token反复拉扯，进而得到下一个词。事实上，为了预测下一个词，这句话的语义、语境，可能受到来自很远位置的文本影响，有时候甚至很远。而我们根据输入文本创建向量组，看看下一个Token是什么的时候，每个向量都是直接从Embedding矩阵中拉出来的。一开始，每个向量都仅仅表示其自身的意思，而不表示语境信息。但是流经了这个网络后，向量就获得了比单个词更加丰富具体的含义。

GPT-3只能处理一定上下文的内容，也就是“上下文长度”，是2048个Token。因此，流经网络的数据有2048列，每列12288维。这相当于限制了，GPT-3预测下一个词是什么的时候，能够结合的文本量。


此时我们看一下模型的输出是什么，稍后来讲Attention。**我们的目标是，输出下一个可能的 Token 的概率分布。**

这需要有2个步骤。首先，我们需要用一个矩阵，将最后一个字的向量（12288维），映射到我们的词表（50267个Token）中去， $\mathbf{Ax}$ 这样，得到的向量 (50257)就代表了词库里一个Token的**值**，我们把这个值再走一遍softmax。

这个矩阵，就是 $W_U$。它具有和Embedding类似的维度，只不过行列对调，也就是行数等于Token数(50k)，列数等于每个Token的维度(12288)。这就是 6.17亿个参数。


!!! note "Softmax"
    **目的是把任何数列转化成合理的概率分布。** 使得最大值接近1，而最小值接近0. $\dfrac{e^{x_1} }{\sum^{N-1}_{n = 0}e^{x_n}}$。
    
    有时候我们会加入一个T，$\dfrac{e^{x_1 / T} }{\sum^{N-1}_{n = 0}e^{x_n / T}}$ 这样，当T较大的时候，低值会被赋予更大的权重，使得分布更加均匀一些。如果T更小，那么较大的值会更占优势。 

通过 $W_U$ 矩阵，我们得到的没有归一化 (softmax) 的每个Token的值，称为 这个词的 Logits.




