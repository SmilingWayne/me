## 方差分析 ANOVA

> Analysis of Variance


- H_0 : \mu_ 1 = \mu_2 = \mu_3
- H_1 : At least one \mu is different 


- e.g. Professor rating (X) with Students' score(Y)

```
Group_1: Rating Excellent(x = 1)

Group_2: Rating Medium(x = 2)

Group_3: Rating Low(x = 3)
```

- 他们的 \mu 是否是相同的？如果相同（接受原假设），说明是否喜欢老师与最终的学生分数无关。（X和Y之间没有关系）
- 做法：
    - 全Y的方差由：X的方差和组内误差方差解释；如果X的方差解释比误差方差的影响还要小，说明是否喜欢老师与分数无关；
    - 组间误差是否比组内误差小； 

###  F分布

- F 检验：样本标准差的平方，两组数据可以得到两个平方，用X的方差去除以误差的方差（注意下自由度的问题），计算出F的值，就是方差的商
    - variation explained by X;
    - variation explained by errors;
    - $F = \dfrac{S^{2}_{1}}{S^{2}_{2}}$ 
- 自由度（见下面一个专题）
- MS：mean square


### 自由度
- 自由度（Degrees of Freedom）

#### 一种便于计算统计量的解释（statistical measures）

- 经典的“你为什么要n-1！”
- Population(总量N) 
    - E(X) = \dfrac{}{}, 总体参数(parameter)
    - \sigma^{2}(x) = \dfrac{}{}


- Sample:
    -  样本，总量n ，样本统计量：
    - \bar{x} （x bar）表示样本期望；
    - $S^{2} (x) = \dfrac{\sum (x_i - \bar{x})^2 }{n - 1}$，这里的 n - 1就是自由度；为什么这里的样本方差会和总体方差不同呢？
    - 我们计算这些样本的统计量，实际上是为了估计(estimate)总体的统计量的，我们大多数时候并不知道总体的情况如何；
    - 我们可以采样！**一个采样形成一个点估计**。样本的方差正是用来估计总体方差的。
    - 问题出现了，如果样本的方差选择用 n 而不是 n - 1，我们常常会低估(underestimate) 总体的方差的真值；如果用 n - 1 就可以纠正这个错误，更加接近真实值；
    - Def: **The number of free-moving observations / elements.**
        - 对于总体来说，可以有N个取值；自由度是N，但是如果我们从中取样之后，计算出给定样本的均值之后，\dfrac{x_1 + x_2 + x_3}{3} = \bar{x}, 实际上只有n - 1个值是“自由”的，因为无论选择哪个数，都可以被这n - 1个数线性表示了。另一个角度说，当样本很大很大( n \to \infty )，N 约等于 N - 1了，严格意义上，此时也不需要考虑自由度了，因为样本就已经变成了整体。
        - ❓那么为什么，在计算均值的时候为什么不需要加上 n - 1: 此时没有任何限制；因为只是一个样本，它确实有n个“自由”的样本个体，所以均值依然是 n 。简单来说，只要你知道了一个样本某类型的均值，那么你就丢失了一个自由度。

#### 一种基于回归分析的解释

- y = b_0 + b_1 x + \mu_i 
-  OLS。先画个散点图。
-  如果想要求b_0, b_1，那么我们最少需要几个观测值才能求出这两个值？
      -  至少需要两个点（不然连不成一个直线）
      -  但是，如果只有两个点，此时这两个点都不是“自由”的，因为当且仅当有了这两个点才能确定这个线，这时候其实不是预测，而是“确定”，根本不需要评估关系强度如何（因为你已经具体确定了）。**此时的自由度是0**。只有有了多于两个点，才能**评估这种相关关系的强度**(assess the strength of relationship)。这时候的自由度就是1.如果是 n = 4，那么自由度就是 2， 以此类推；也就是和独立变量的个数 x相关，（回归变量个数？number of regressors），$d.f. = n - (k + 1)$
      -  如果我们有两个独立变量， $y = b_0 + b_1 x_1 + b_2 x_2 + \mu$，此时是一个回归平面；此时需要的最少的样本数是3（抽象一下，如果只有两个点，连成一个线，那么有无数个面可以过这个线——除非你再确定一个点，让这个平面也过这个点，才能唯一确定一个平面；
      -  拓展：回归分析中 $R^2$ 情况的分析


## $R^2$ 的解释
- 被模型解释的Y的部分(fraction of variation explained in Y)
- $SS_{Total} = \sum (y_i - \bar{y})^2$ = total variation in Y
- $SS_{Model} = \sum (\hat{y_i} - \bar{y})^2$ variation in y explained by X 
- SS Error + SS model = SS total ;
- $R^2 =  \dfrac{SS_{Model}}{SS_{Total}}$
  - R^2 = 0，意味着无法用X来解释Y；（散点图的Y均值是一条和X轴平行的直线）
  - R^2 = 1，意味着所有的y（的误差）都可以被用X解释；
  - R^2 = 0.8，意味着80%的Y的误差可以用X解释； （coefficient of determination）
    - 皮尔逊相关系数的平方？
    - hat 这个符号就用来表示剔除了误差项，仅仅从x回归得到的y的期望值(expected value)；
- 如果是多元的，一样，$y_i = b_0 + b_1x_1+ b_2x_2 + ... + u_i$
    -  $R^2 = \dfrac{\sum (\hat{y_i} - \bar{y})^2}{\sum (y_i - \bar{y})^2}$
- 这时候要回到刚开始的自由度。如果单regressor的情况下，此时有两个观测，自由度是0，此时的R^2 = 1（因为这两个点完全解释了这一条直线！但是这并不好！因为它不自由！）；如果有三个观测，自由度是1，$R^2$可能降到$0.8$，但是此时的模型更好了，因为有了自由度，对模型关系评价的强度就更高了；
  
## Adjusted $R^2$ 的解释 
- 正如上面展开说的那样，其实$R^2$本身是很令人疑惑的，因为过高的$R^2$可能说明恰恰是缺少了自由度的。
    - 比如三个点确定一个面，那这个面唯一确定，而不能反映关系

1. more regressors to explain Y, 更多回归变量，即使这两个不相关，总有一部分$Y$能被$X$所解释；R^2 会变大；
2. more regressors 也意味着自由度越来越低；更低的自由度往往意味着更高的$R^2$；
      1. regressors 就是, $y = b_0 + b_1 x_1 + b_2 x_2.. + u_i$，中的$x$的个数（也就是2个）
- 综上2点，不能仅仅看R^2
- 所以我们引入了adjusted R^2的概念，如果1的作用大于2的作用，那么\bar{R^2} 就会增加，反之减少；意味着，这个adjusted R^2 只在“你加入了一些解释变量，而且这些解释变量的解释力度大于自由度减少带来的解释力度，也就是更好滴提升模型预测能力”的时候才增加；