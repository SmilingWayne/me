# 笔试


## 算法基础

!!! note "各个排序算法的稳定性"
    TBD

!!! note "完全二叉树的定义"

## 数学基础

对于随机变量X，其分布函数为F(x)，则下述函数中可能为分布函数的选项有？

$F^3(x)$，$F(3x)$，$3F(x)$，$1/F(x)$

答案：AB。分布函数有以下基本特征：
    
1.在定义域内，$F(x)$ 单调不减，且在定义域内任意一点右连续；2.对于分布函数F(x)的极限，有 $F(-\infty)=0，F(+\infty)=1$；3. $F(x)$ 取值在[0,1]范围内。A,B两选项满足以上要求；


## 深度学习/机器学习

!!! note "在深度学习中，以下哪些因素可能导致Out Of Distribution（OOD）问题的发生？"
    ✅ 训练数据与实际应用场景差异较大
    
    ✅ 数据标签的错误
    
    ✅ 训练数据中存在噪声
    
    ❌ 模型过于复杂

!!! question "准确率、召回率、精准率有什么区别"
    `recall`：召回率：实际为正的样本中被预测为正样本的概率（不放过坏用户，召回率越高，代表实际坏用户被预测出来的概率越高，它的含义类似：宁可错杀一千，绝不放过一个。场景：不放过假阳性）

    `accuracy`：准确率：预测正确的结果占所有样本的百分比（无法处理样本不平衡的问题）

    `precision`：精确率：所有被预测为正的结果中实际为正的百分比（包括了负样本）（场景：需要减少假阳性）


!!! note "什么是过拟合？什么是正则化？L1和L2正则化的区别"
    过拟合（`Overfitting`）是机器学习中一个常见的问题，指的是模型**在训练数据上表现得过于优秀**，以至于它开始记住训练数据中的噪声和细节，而**不是学习到数据背后的真正规律。** 这导致模型**在新数据上的表现不佳**。

    - **正则化就是减少过拟合的一种方法**。通过在模型的损失函数中添加一个额外的惩罚项，鼓励模型学习更小的权重值来限制模型的复杂度。

    - L1正则化：向损失函数添加权重的绝对值之和作为惩罚项来工作。这有助于产生稀疏权重矩阵，即许多权重会变为零，从而实现特征选择的效果。

    - L2正则化：向损失函数添加权重平方的和作为惩罚项来工作。L2正则化倾向于让权重均匀地小，而不是像L1正则化那样将它们压缩为零。

!!! question "还有哪些解决过拟合的方法？"
    - `dropout`：在训练过程中随机丢弃（置零）一部分神经元的输出，迫使网络学习冗余的特征，从而提高泛化能力。
    - `early-stopping`：在训练过程中监控验证集的性能，当验证集的性能不再提升时停止训练，以避免模型在训练集上过度拟合。
    - `Bagging`或`Boosting`，使用多个模型的预测结果进行集成，可以提高模型的泛化能力。

!!! question "LightGBM和XGBoost的区别是什么？"
    TBD.

!!! note "XGBoost和GBDT的区别？优化了哪些地方？有什么作用？"
    TBD.

!!! question "XGB和LGBM对缺失值的处理有什么不同"
    TBD.

!!! question "Dropout在预测和训练阶段有什么区别？"
    TBD.

!!! question "什么是ReLU激活函数？还有哪些激活函数？特点是什么？作用是什么？（四连问）"
    TBD.

!!! question "如何解决梯度消失或者梯度爆炸问题？"
    TBD.

!!! note "二分类评估指标有哪些"
    TBD.

!!! note "监督学习、无监督学习、半监督学习的区别"
