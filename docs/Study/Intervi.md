# 一些面试题目


!!! question "准确率、召回率、精准率有什么区别"
    recall：召回率：实际为正的样本中被预测为正样本的概率（不放过坏用户，召回率越高，代表实际坏用户被预测出来的概率越高，它的含义类似：宁可错杀一千，绝不放过一个。场景：不放过假阳性）

    accuracy：准确率：预测正确的结果占所有样本的百分比（无法处理样本不平衡的问题）

    precision：精确率：所有被预测为正的结果中实际为正的百分比（包括了负样本）（场景：需要减少假阳性）


!!! note "什么是过拟合？什么是正则化？L1和L2正则化的区别"
    过拟合（Overfitting）是机器学习中一个常见的问题，指的是模型**在训练数据上表现得过于优秀**，以至于它开始记住训练数据中的噪声和细节，而**不是学习到数据背后的真正规律。** 这导致模型**在新数据上的表现不佳**。

    - 正则化就是减少过拟合的一种方法。通过在模型的损失函数中添加一个额外的惩罚项，鼓励模型学习更小的权重值来限制模型的复杂度。

    - L1正则化：向损失函数添加权重的绝对值之和作为惩罚项来工作。这有助于产生稀疏权重矩阵，即许多权重会变为零，从而实现特征选择的效果。

    - L2正则化：向损失函数添加权重平方的和作为惩罚项来工作。L2正则化倾向于让权重均匀地小，而不是像L1正则化那样将它们压缩为零。

!!! question "还有哪些解决过拟合的方法？"
    - dropout：在训练过程中随机丢弃（置零）一部分神经元的输出，迫使网络学习冗余的特征，从而提高泛化能力。
    - early-stopping：在训练过程中监控验证集的性能，当验证集的性能不再提升时停止训练，以避免模型在训练集上过度拟合。
    - 使用多个模型的预测结果进行集成，如Bagging或Boosting，可以提高模型的泛化能力。


!!! question "LightGBM和XGBoost的区别是什么？"
    Light GBM是一种高效的梯度提升框架，它使用了基于树的学习算法。
    
    1. 这种方法通过将连续特征的值离散化到有限数量的bins中，从而减少了模型的搜索空间，加速了训练过程。
    2. 梯度单边采样（Gradient-Based One-Side Sampling, GOSS）：LightGBM采用GOSS算法，在训练时只关注那些梯度较大的样本，从而减少了计算量并保持了数据分布的一致性。
    3. 互斥特征捆绑：通过捆绑那些几乎不会同时发生的互斥特征，LightGBM降低了特征的数量，从而减少了模型的复杂度和计算量。
    4. 与XGBoost的Level-wise（按层生长）策略不同，LightGBM采用Leaf-wise策略，每次选择增益最大的叶子进行分裂，这通常可以更快地减少误差。
    5. LightGBM原生支持并行学习，包括特征并行和数据并行，它通过减少通信开销和优化Cache命中率，提高了并行处理的效率。

!!! note "XGBoost和GBDT的区别？优化？"
    是一种高效的机器学习算法，它是基于梯度提升决策树（GBDT）的实现。XGBoost在GBDT的基础上做了很多优化，使其在处理大规模数据集时更加高效、灵活并且具有更好的预测性能。

    1. 梯度提升框架：XGBoost使用了梯度提升算法，这是一种集成学习技术，通过组合多个弱预测模型（如决策树）来构建一个强预测模型。
    2. 树模型优化：XGBoost优化了树模型的构建过程，采用了正则化项来防止树模型过拟合，并且支持用户自定义优化目标和评估准则。
    3. XGBoost在生长树时，会采用深度优先的方法，并在增长到最大深度后进行剪枝，这与GBDT的贪心算法有所不同。

!!! question "XGB和LGBM对缺失值的处理有什么不同"


!!! question "Dropout在预测和训练阶段有什么区别？"
    Dropout是一种在训练阶段使用的正则化技术，主要用于深度学习中的神经网络，以减少过拟合。以下是Dropout在训练阶段和预测阶段的主要区别：

    1. 随机失活：在每次迭代或每次前向传播时，Dropout随机地将一些神经元的输出置为零，这个概率由超参数决定（例如，0.5表示平均有50%的神经元会被置零）。
    
    2. 减少共适应：通过随机关闭神经元，Dropout减少了神经元之间复杂的共适应关系，迫使网络学习更加鲁棒的特征表示。
    
    全神经元激活：在预测阶段，不使用Dropout。所有的神经元都处于激活状态，参与前向传播。

!!! question "什么是ReLU激活函数？"
    修正线性单元，把输入值限制在一个非负的范围内。

    $\leq 0, 0 ; \geq 0, \max (0, x)$

!!! question "梯度爆炸或者为0的时候，如何解决？"

    解决梯度爆炸的方法：

    1. 使用合适的激活函数ReLU、Leaky ReLu；
    2. 梯度剪裁（Gradient Clipping）： 在每个训练步骤后，检查梯度的范数（或某些权重的范数），如果超过了某个阈值，就将梯度进行缩放，从而限制梯度的大小。
    3. 使用批归一化（Batch Normalization），批归一化在每层的输入上对数据进行标准化，可以减少梯度的变化，有助于稳定训练。
    4. 改进优化器：使用RMSProp、Adam等自适应学习率优化算法，它们可以根据历史梯度动态调整学习率，有助于更好地控制参数更新的尺度。
   
    ----

    解决梯度消失的方法：

    5. 更换激活函数：避免使用如sigmoid和tanh这样的饱和激活函数，因为它们在两端区域的导数接近于零。
    6. 残差连接（Residual Connections）：引入残差块结构，允许梯度绕过某些层直接传递到更早的层，显著缓解梯度消失问题，这一技巧在ResNet等网络架构中发挥了重要作用。
    7. 选择合适的权重初始化策略，比如（Xavier初始化），

!!! note "搜广推业务的流程是什么样的"
    - 搜广推业务，即搜索引擎（Search）、广告（Advertising）和推荐系统（Recommendation）的合称，是互联网领域中非常核心的业务场景。它们的流程通常包括以下几个关键步骤：

    1. 数据收集与处理：
    
    > 收集用户的行为数据，如搜索查询、点击、浏览历史等。
    >
    > 对数据进行清洗、处理和特征工程，以便于模型训练
    
    2. 用户画像构建：
    
    > 根据用户的历史行为和偏好构建用户画像，这有助于理解用户需求和提升推荐的相关性。
    
    3. 内容分析：
    > 对内容（如商品、文章、广告等）进行分析，提取特征，构建内容画像。
    
    4. CTR预估模型构建：
    
    > 使用机器学习或深度学习算法构建点击率预估（CTR）模型，预测用户对特定内容的点击概率。
    
    5. 排序与推荐：
    
    > 根据CTR预估模型的输出对内容进行排序，决定展示给用户的顺序。
    
    6. 在线服务与实时反馈：
    
    > 实时收集用户对推荐内容的反馈，如点击、停留时间等，快速调整推荐策略。
    
    7. A/B测试：
    
    > 对不同的推荐策略或模型进行A/B测试，评估效果并选择最佳方案。
    
    8. 业务指标优化：
    
    > 根据业务目标，如提高用户活跃度、增加广告收入等，持续优化模型和推荐策略。
    模型更新与维护：
    
    > 定期更新模型以适应用户行为的变化，并进行模型维护。
    
    9. 收益优化：
    > 在广告业务中，还需要考虑如何优化广告的展示和点击收益，可能涉及到更复杂的计费和排序逻辑。

    搜广推业务的流程是动态的，需要不断地根据用户行为和市场变化进行调整和优化。此外，这个流程也涉及到大量的数据处理和机器学习模型训练工作，需要数据工程师、算法工程师和产品经理紧密合作

!!! note "激活函数的作用是什么"
    **非线性映射**：激活函数为神经网络引入非线性，允许网络学习复杂的函数映射。如果没有非线性激活函数，即使网络有多个层，最终也等同于一个单层线性模型，这严重限制了网络的表达能力。
    
    **控制神经元的激活**：激活函数决定在给定的输入下神经元是否应该被激活。例如，ReLU（Rectified Linear Unit）在输入大于零时激活神经元。
    
    **损失函数的梯度计算**：在反向传播过程中，激活函数的导数用于计算梯度。某些激活函数（如ReLU）的导数简单且易于计算，这有助于提高训练效率。
    
    缓解梯度消失问题：某些激活函数（如ReLU）可以帮助缓解梯度消失问题，即在深度神经网络中梯度随着层数增加而迅速减小的问题。实现不同的网络行为：
    
    门控机制：在循环神经网络（RNN）中，激活函数如sigmoid和tanh被用于门控机制，控制信息的流入和流出。
    
    ----

    ReLU（Rectified Linear Unit）：f(x) = max(0, x)，计算简单，训练效率高，是目前最流行的激活函数之一。
    Sigmoid：f(x) = 1 / (1 + exp(-x))，将输出压缩到(0, 1)区间，常用于二分类问题。
    Tanh（双曲正切）：f(x) = 2σ(2x) - 1，将输出压缩到(-1, 1)区间，常用于数据的归一化。


!!! note "二分类评估指标有哪些"

    